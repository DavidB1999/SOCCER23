{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29cc0fe",
   "metadata": {},
   "source": [
    "# Percentile calculation\n",
    "We need:\n",
    "+ the sorted list of all frames \n",
    "+ the limits of percentile 80 and the 8 groups as index\n",
    "+ the boarder values of percentile 80 and the 8 groups\n",
    "\n",
    "Preferably this is done with no additional random sorting of frames with identical success scores, since that would be redundant considering that we later split goals with a certain success score to all groups that contain some frames with that success score\n",
    "\n",
    "# Over and under the 80th percentile\n",
    "### we exclude values above 100 (false values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary with one array for each dataset (area & interval)\n",
    "# These arrays contain each frame (as a list) entailing:\n",
    "# Success score, home (-3) or away (-1), string filename, minute and second, the half and the time as a string in a practical format\n",
    "# The array for every dataset is sorted by Success score\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "        \n",
    "#1:\n",
    "game_data = (r'SUCCESS_SCORES')\n",
    "\n",
    "# import the datasets\n",
    "with open(\"Pickles/Datasets.pkl\", \"rb\") as l:\n",
    "    datasets = pickle.load(l)\n",
    "\n",
    "\n",
    "final_dict100 ={}                                               # create an empty dictionary\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    final_dict100[datasets[i]] = []                              # datasets as keys\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dirMD in os.listdir(game_data):\n",
    "        locMD = os.path.join(game_data, dirMD)\n",
    "        for dirM in os.listdir(locMD):\n",
    "            locM = os.path.join(locMD, dirM)\n",
    "            # os.chdir(locM)                                         # setting the working directory to data folder causes chaos with checkpoints\n",
    "            for file in os.listdir(locM):\n",
    "                if file.startswith('zero') & file.endswith(\"postprocessed.csv\"):\n",
    "                    if dataset in file:                                # Check if its a file for the current dataset (area & interval length)\n",
    "                        with open(os.path.join(locM, file), 'r') as f:\n",
    "                            reader = csv.reader(f, delimiter =',')\n",
    "                            for row in reader:\n",
    "                                if row != []:\n",
    "                                    if float(row[-1]) < 100.1:  # only if sensible success score < 100\n",
    "                                        final_dict100[dataset].append([float(row[-1]), -1, file, row[0], row[1]])     # index of filename needs to be adapted later\n",
    "                                    if float(row[-3]) < 100.1:\n",
    "                                        final_dict100[dataset].append([float(row[-3]), -3, file, row[0], row[1]])     # content of dictionary see above (1.)\n",
    "                                                                                                        # (-3 = home, -1 = away),\n",
    "                                \n",
    "for dataset in datasets:\n",
    "    for i in range(len(final_dict100[dataset])):\n",
    "        if 'HT1' in final_dict100[dataset][i][2]:                    # appending a half time indicator \n",
    "            final_dict100[dataset][i].append(float(1))               # if HZ1 is in file name we append a one\n",
    "        elif 'HT2' in final_dict100[dataset][i][2]:                  # if HZ2 is in file name \n",
    "            final_dict100[dataset][i].append(float(2))               # we append a 2\n",
    "        else:\n",
    "            final_dict100[dataset][i].append('Error')                # if none we append Error (which we use to check later)\n",
    "        \n",
    "        if len(str(final_dict100[dataset][i][4])) == 1:              # if the seconds is just one digit (less than 10s) we need\n",
    "            proxy_s = str(0) + str(final_dict100[dataset][i][4])     # to add a 0 to get a format that allows us to compare with SOCCER output\n",
    "        else:\n",
    "            proxy_s = final_dict100[dataset][i][4]\n",
    "            \n",
    "        if final_dict100[dataset][i][5] == 1:                        # if it is the first half we just combine minutes and seconds\n",
    "            final_dict100[dataset][i].append(\".\".join([final_dict100[dataset][i][3], proxy_s]))   # into one variable\n",
    "        elif final_dict100[dataset][i][5] == 2:                      # if it is the second half we add 45 to the minutes\n",
    "            proxy_m = str(int(final_dict100[dataset][i][3]) + 45)      # and then combine them the same way\n",
    "            final_dict100[dataset][i].append(\".\".join([proxy_m, proxy_s]))\n",
    "             \n",
    "    final_dict100[dataset].sort()               # sort by success score primarily and the random order secondly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7445d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Pickles/AllDatapoints100.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(final_dict100, cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict100['30m_100s'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12496b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy alternative to get percentile border values\n",
    "# Saving the whole bunch to use the 80th percentile values later\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "percentiles_10 = {}\n",
    "for dataset in datasets:\n",
    "    percentiles_10[dataset] = []\n",
    "    values = []\n",
    "    for line in final_dict100[dataset]:\n",
    "        values.append(line[0])\n",
    "    percentiles_10[dataset] = np.percentile(values, [0,10,20,30,40,50,60,70,80,90,100])          # saving the boarder values in a dicitonary\n",
    "\n",
    "\n",
    "with open(\"Pickles/Boarder_values_10_e100.pkl\", \"wb\") as dict:                # saving the boarder values in a dicitonary\n",
    "    pickle.dump(percentiles_10, dict)\n",
    "        \n",
    "percentiles_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dictionary into over and under 80 resulting in twice as many lists as datasets (dataset_over, dataset_under)\n",
    "# simultaneously storing limit indexes \n",
    "\n",
    "import math\n",
    "split_dict = {}                           # dicitonary where we split the datasets in over and under the 80 --> 2 arrays (over and under) per dataset (area & interval)\n",
    "limits_80 = {}                               # simultaneously storing the limits (index) as well as the total length for use later\n",
    "for dataset in datasets:\n",
    "    limits_80[dataset] = []\n",
    "    length = (len(final_dict100[dataset]))           # the 80th percentile (when sorted) is just the borderline of 80% of the values\n",
    "    print(length)\n",
    "    percentile = length * 0.8                  # get 80% length of the values\n",
    "    #print(percentile)\n",
    "    upper =round(percentile)          # substract 1 since index counting starts with zero and add one because the last one is not included :5 --> 1,2,3,4\n",
    "    print(upper)\n",
    "    lower = round(percentile)          # upper and lower are the same index as the last index is not included when selecting :5 --> 1,2,3,4\n",
    "    print(lower)\n",
    "    limits_80[dataset].append(upper)       # append the limit index\n",
    "    limits_80[dataset].append(length)      # and the total length\n",
    "    split_dict[\"_\".join([dataset, 'under'])] = final_dict100[dataset][:upper]\n",
    "    split_dict[\"_\".join([dataset, 'over'])] = final_dict100[dataset][lower:]\n",
    "    \n",
    "with open(\"Pickles/index_limits_80_e100.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(limits_80, dict)\n",
    "\n",
    "limits_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af924eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6271b5",
   "metadata": {},
   "source": [
    "# 8 groups &rarr; percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8270e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ccb132",
   "metadata": {},
   "source": [
    "**length** return the **number of cases** which is **one more** than the last index since 0 is the index of the first case. But since index :x return everything up to x but **excluding x** the length should be the upper limit of the percentile 87.5 - 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into the 8 groups --> 8 lists (by percentiles) per dataset (area & interval)\n",
    "# saving the index limits and the success score boarder values\n",
    "\n",
    "import math\n",
    "\n",
    "percentiles = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]\n",
    "split8_dict = {}\n",
    "limits_8 = {}\n",
    "boarder_values_8 = {}\n",
    "lower_lim = 0\n",
    "for dataset in datasets:\n",
    "    limits_8[dataset] = []\n",
    "    boarder_values_8[dataset] = []\n",
    "    lower_lim = 0\n",
    "    length = (len(final_dict100[dataset]))           # the 80th percentile (when sorted) is just the borderline of 80% of the values\n",
    "    for p in range(len(percentiles)):\n",
    "        percentile = round(length * percentiles[p])            # get percentile, round uses symetrical rounding (.5 --> up or down to go to even number (3.5 --> 4, 2.5 --> 2))\n",
    "        upper_lim = percentile                                 # this becomes the upper limit\n",
    "        limits_8[dataset].append(upper_lim)\n",
    "        boarder_values_8[dataset].append(final_dict100[dataset][upper_lim-1][0])\n",
    "        print(str(lower_lim) +' - ' +  str(upper_lim))         # just to check we print our intervals\n",
    "        \n",
    "        split8_dict[\"_\".join([dataset, str(percentiles[p])])] = final_dict100[dataset][lower_lim:upper_lim]     # and then split by these intervals\n",
    "#         if p == 7:                         \n",
    "#             lower_lim = 0                                      # see below except if its the last round in this loop then we need to start with 0 next time\n",
    "#         else:\n",
    "#             lower_lim = upper_lim                              # for the next round in this loop the upper limit of this round become the lower limit for the next\n",
    "        lower_lim = upper_lim                                    # solved it more elegantly with settting lower_lim in the loop on layer above\n",
    "limits_8 \n",
    "\n",
    "with open(\"Pickles/index_limits_8_e100.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(limits_8, dict)\n",
    "\n",
    "with open(\"Pickles/Boarder_values_8_e100.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(boarder_values_8, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boarder_values_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2acafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "split8_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in split8_dict.keys():\n",
    "    print(key)\n",
    "    print(len(split8_dict[key]))     # the subsets of each dataset should have the same length which is rouned to 1/8th\n",
    "# see next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in final_dict100:\n",
    "    print(len(final_dict100[elem]))\n",
    "    print(len(final_dict100[elem]) / 8)\n",
    "    \n",
    "# this is how long the subsets should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76650e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum, maximum and most importantly mean success score for every percentile group of every dataset\n",
    "# means stored as csv\n",
    "\n",
    "import pandas as pd\n",
    "percentiles = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]\n",
    "dfs = {}\n",
    "means_dict = {}\n",
    "    \n",
    "for data in split8_dict:                          # for every subset (8 x #datasets)\n",
    "    dfs[data] = pd.DataFrame(split8_dict[data], columns=['Success Score', 'Home_away', 'csv_file', 'min', 'sec', 'half', 'time_weird_format'])  # creating a dictionary of dataframes\n",
    "    minimum = dfs[data]['Success Score'].min()\n",
    "    maximum = dfs[data]['Success Score'].max()\n",
    "    mean = dfs[data]['Success Score'].mean()\n",
    "    means_dict[data] = mean\n",
    "    print(data + ';' + str(minimum) + '-' +str(maximum) + ';' +str(mean))\n",
    "\n",
    "means_df = pd.DataFrame()\n",
    "   \n",
    "for dataset in datasets:\n",
    "    mean_v = []\n",
    "    for i in percentiles:\n",
    "        mean_v.append(means_dict[\"_\".join([dataset, str(i)])])\n",
    "    means_df[dataset] = mean_v\n",
    "    \n",
    "\n",
    "means_df.to_csv('Results/Mean_SuccessScores_8_e100.csv')    \n",
    "means_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eeb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open(\"Pickles/80_20_split_e100.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(split_dict, cdict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475756aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Pickles/AllDatapoints_e100.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(final_dict100, cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "with open(\"Pickles/8_split_e100.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(split8_dict, cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89c7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodlight_env",
   "language": "python",
   "name": "floodlight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
