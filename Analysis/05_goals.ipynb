{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23b7a3b",
   "metadata": {},
   "source": [
    "# 80 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91299fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the above and below 80th percentile split data\n",
    "with open('Pickles/80_20_split_e100.pkl', 'rb') as file:\n",
    "    split80_20 = p.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split80_20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the goal information\n",
    "goals = pd.read_csv('Output/GoalOverview.csv')\n",
    "goals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5994fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "games_included = []\n",
    "\n",
    "game_data = (r'SUCCESS_SCORES')\n",
    "\n",
    "for dirMD in os.listdir(game_data):\n",
    "        locMD = os.path.join(game_data, dirMD)\n",
    "        for dirM in os.listdir(locMD):\n",
    "            games_included.append(dirM)\n",
    "games_included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds under 10 need to be formatted to string with 0x instead of x which turns into --> x0 as a string\n",
    "seconds = []\n",
    "for i in range(len(goals.Second)):\n",
    "    if goals.Second[i] < 10:\n",
    "        seconds.append('0' + str(goals.Second[i]))\n",
    "    else:\n",
    "        seconds.append(str(goals.Second[i]))\n",
    "        \n",
    "goals['Seconds2'] = seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an additional time string and minute that goes up to 90 instead of 45 for each half\n",
    "\n",
    "goals['time_string_H'] = goals['Minute'].astype(str) + '.' + goals['Seconds2']\n",
    "goals['Minute2'] = goals['Minute']\n",
    "goals['Minute2'].loc[goals['Half'] == 2] = goals['Minute'] + 45\n",
    "\n",
    "goals['time_string'] = goals['Minute2'].astype(str) + '.' + goals['Seconds2'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# scoring_ha = which team scored\n",
    "\n",
    "goals.loc[goals['Scoring_Team'] == goals['Home'], 'Scoring_ha'] = -3\n",
    "goals.loc[goals['Scoring_Team'] == goals['Away'], 'Scoring_ha'] = -1\n",
    "\n",
    "goals.to_excel(\"Output/goal_table.xlsx\")\n",
    "goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the relevant goals (Open play)\n",
    "\n",
    "print(type(goals.InBox))\n",
    "print(goals.InBox.unique())\n",
    "\n",
    "print(type(goals.OpenPlay))\n",
    "print(goals.OpenPlay.unique())\n",
    "\n",
    "\n",
    "#  516 open play goals = relevant goals\n",
    "goals.loc[(goals.OpenPlay == \"True\") & (goals.DFL_Name.isin(games_included))]\n",
    "\n",
    "# 450 open play goals from within the box (relevant for the 16m Success Scores)\n",
    "goals.loc[(goals.OpenPlay == \"True\") & (goals.InBox == True) & (goals.DFL_Name.isin(games_included))].reset_index()\n",
    "# reset index because i'll be loopig over the index \n",
    "goals_rel = goals[(goals.OpenPlay == \"True\") & (goals.DFL_Name.isin(games_included))].reset_index()\n",
    "goals_rel.drop(['index', 'Unnamed: 0'], axis=1, inplace = True)\n",
    "\n",
    "goals_rel.to_excel('Output/Goal_Table.xlsx')\n",
    "\n",
    "# create column to store the dictionary frame related to the goal, to check which goals are missing a frame\n",
    "goals_rel['dict_entry_16m_100s'] = 'Missing'\n",
    "goals_rel['dict_entry_16m_300s'] = 'Missing'\n",
    "goals_rel['dict_entry_16m_500s'] = 'Missing'\n",
    "goals_rel['dict_entry_30m_100s'] = 'Missing'\n",
    "goals_rel['dict_entry_30m_300s'] = 'Missing'\n",
    "goals_rel['dict_entry_30m_500s'] = 'Missing'\n",
    "\n",
    "goals_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f910610",
   "metadata": {},
   "source": [
    "## Mean success score for goals and no goals\n",
    "### We take all 516 open play goals as goals for 30m intervals and only the 450 open play within the box for 16m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import pickle as p\n",
    "\n",
    "# load complete data\n",
    "with open('Pickles/AllDatapoints_e100.pkl', 'rb') as file:\n",
    "    full_dict = p.load(file)\n",
    "\n",
    "datasets = list(full_dict.keys())                            # datasets\n",
    "goal_dict = {}                                               # dictionary for the split of succcess scores in goal and no goal\n",
    "df_SS_goal = pd.DataFrame(index=['Goal', 'No_goal'])         # df for means\n",
    "# thought about looking for the fitting data for each goal \n",
    "# decided to keep it as it is looking whether a goal matches the data\n",
    "# have to check all of the data anyway\n",
    "for dataset in datasets:\n",
    "    goal_dict[\"_\".join([dataset, 'GOAL'])] = []                  # for every previous dataset in my dictionary i now want two: \n",
    "    goal_dict[\"_\".join([dataset, 'NOgoal'])] = [] \n",
    "    for frame in range(len(full_dict[dataset])):                # for every frame\n",
    "        if dataset.startswith('30'):                                                # for the 30m intervals we don't need to check whether the goal was scored from within the penalty area\n",
    "            if full_dict[dataset][frame][6] in goals_rel.time_string.values:        # to identify goals the time has to match\n",
    "                index_list = np.where(goals_rel[\"time_string\"].values == full_dict[dataset][frame][6])      # all those which match by the time stamp\n",
    "                for i in index_list[0]:\n",
    "                    if goals_rel.DFL_Name[i] in full_dict[dataset][frame][2] and goals_rel.Scoring_ha[i] == full_dict[dataset][frame][1] and goals_rel.Half[i] ==full_dict[dataset][frame][5]:       # have to also match by the match id and the team (scoring team) and the half (46 = HT1 vs 46 = HT2 min 1)\n",
    "                        goal_dict[\"_\".join([dataset, 'GOAL'])].append(full_dict[dataset][frame][0])         # if yes we append to the goal data set\n",
    "                        if dataset == '30m_100s':    # for now i just store the goal related frame only for the 16m_100s seocond data\n",
    "                            goals_rel.dict_entry_30m_100s[i] = full_dict[dataset][frame]\n",
    "                        elif dataset == '30m_300s':\n",
    "                            goals_rel.dict_entry_30m_300s[i] = full_dict[dataset][frame]\n",
    "                        elif dataset == '30m_500s':\n",
    "                            goals_rel.dict_entry_30m_500s[i] = full_dict[dataset][frame]\n",
    "                        # print(split80_20[dataset][frame])\n",
    "                        # print(dataset)\n",
    "                    else:\n",
    "                        goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])      # else we append to the no goal data set\n",
    "            else: \n",
    "                goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])              # else we append to the no goal data set\n",
    "        elif dataset.startswith('16'):                                               # for 16m intervals we have to check if scored from within the penalty are\n",
    "            if full_dict[dataset][frame][6] in goals_rel.time_string[goals_rel.InBox==True].values:        # to identify goals the time has to match (==True --> from within the box)\n",
    "                #print(full_dict[dataset][frame])\n",
    "                index_list = np.where(goals_rel[\"time_string\"].values == full_dict[dataset][frame][6])      # all those which match by the time stamp\n",
    "                #print(index_list)\n",
    "                for i in index_list[0]:\n",
    "                    if goals_rel.DFL_Name[i] in full_dict[dataset][frame][2] and goals_rel.Scoring_ha[i] == full_dict[dataset][frame][1] and goals_rel.Half[i] ==full_dict[dataset][frame][5] and goals_rel.InBox[i]==True:       # have to also match by the match id and the team (scoring team) and the half and check inbox in case of two indeces in index list\n",
    "                        goal_dict[\"_\".join([dataset, 'GOAL'])].append(full_dict[dataset][frame][0])         # if yes we append to the goal data set\n",
    "                        if dataset == '16m_100s':    # for now i just store the goal related frame only for the 16m_100s seocond data\n",
    "                            goals_rel.dict_entry_16m_100s[i] = full_dict[dataset][frame]\n",
    "                        elif dataset == '16m_300s':\n",
    "                            goals_rel.dict_entry_16m_300s[i] = full_dict[dataset][frame]\n",
    "                        elif dataset == '16m_500s':\n",
    "                            goals_rel.dict_entry_16m_500s[i] = full_dict[dataset][frame]\n",
    "                            \n",
    "                            \n",
    "\n",
    "                        # print(split80_20[dataset][frame])\n",
    "                        # print(dataset)\n",
    "                    else:\n",
    "                        goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])      # else we append to the no goal data set\n",
    "            else: \n",
    "                goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])              # else we append to the no goal data set\n",
    "\n",
    "    print(dataset)\n",
    "    print(mean(goal_dict[\"_\".join([dataset, 'GOAL'])]))\n",
    "    print(mean(goal_dict[\"_\".join([dataset, 'NOgoal'])]))\n",
    "    df_SS_goal[dataset] = [mean(goal_dict[\"_\".join([dataset, 'GOAL'])]), mean(goal_dict[\"_\".join([dataset, 'NOgoal'])])]\n",
    "    df_SS_goal\n",
    "    with open('Pickles/Goal_Dict_e100.pkl', 'wb') as cdict:\n",
    "        p.dump(goal_dict, cdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dict['16m_100s_GOAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f99d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dict['16m_300s_GOAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dict['30m_100s_GOAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dict['30m_300s_GOAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goal_dict['16m_100s_GOAL']))\n",
    "print(len(goal_dict['16m_100s_NOgoal']))\n",
    "print(len(goal_dict['16m_100s_GOAL']) + len(goal_dict['16m_100s_NOgoal']))\n",
    "print(len(full_dict['16m_100s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goal_dict['30m_100s_GOAL']))\n",
    "print(len(goal_dict['30m_100s_NOgoal']))\n",
    "print(len(goal_dict['30m_100s_GOAL']) + len(goal_dict['16m_100s_NOgoal']))\n",
    "print(len(full_dict['30m_100s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goal_dict['16m_300s_GOAL']))\n",
    "print(len(goal_dict['16m_300s_NOgoal']))\n",
    "print(len(goal_dict['16m_300s_GOAL']) + len(goal_dict['16m_100s_NOgoal']))\n",
    "print(len(full_dict['16m_300s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff228284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goal_dict['30m_300s_GOAL']))\n",
    "print(len(goal_dict['30m_300s_NOgoal']))\n",
    "print(len(goal_dict['30m_300s_GOAL']) + len(goal_dict['16m_100s_NOgoal']))\n",
    "print(len(full_dict['30m_300s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_goal.to_csv('Results/Success_Score_goal_e100.csv')\n",
    "df_SS_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_perc = pd.DataFrame(index=['Goal', 'No_goal'])\n",
    "for dataset in datasets:\n",
    "    perc_mean = []\n",
    "    \n",
    "    for v in range(0,2):\n",
    "        perc = []\n",
    "        i = 0\n",
    "        for line in full_dict[dataset]:\n",
    "            i += 1\n",
    "            if line[0] > (df_SS_goal[dataset][v]-0.1) and line[0] < (df_SS_goal[dataset][v]+0.1):\n",
    "                print(dataset)\n",
    "                print(line)\n",
    "                print(len(full_dict[dataset]))\n",
    "                print(i/len(full_dict[dataset]))\n",
    "                print(i)\n",
    "                print('END OF THIS BLOCK')\n",
    "                print('')\n",
    "                perc.append(i/len(full_dict[dataset]))\n",
    "        perc_mean.append(mean(perc))\n",
    "    df_SS_perc[dataset] = perc_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_perc.to_csv('Results/Success_Score_percentile_e100.csv')\n",
    "df_SS_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a column to which subset each frame /SS belongs\n",
    "# so that i can then do what MS did and split goals scored on a frequent SS in different groups\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open('Pickles/AllDatapoints_e100.pkl', 'rb') as file:            # import the dictionary where data is not split by group or 80 percentile\n",
    "    full_dict = p.load(file)\n",
    "\n",
    "with open('Pickles/index_limits_80_e100.pkl', 'rb') as file:                 # import the boarders /limits (index) for over under 80\n",
    "    limits_80 = p.load(file)\n",
    "\n",
    "with open('Pickles/index_limits_8_e100.pkl', 'rb') as file:                 # import the boarders /limits (index) for the 8 groups\n",
    "    limits_8 = p.load(file)\n",
    "    \n",
    "success_scores = {}                                      # create a dictionary to sort the success scores\n",
    "\n",
    "datasets = list(full_dict.keys())                    # for all datasets by area & interval (not split)  \n",
    "for dataset in datasets:\n",
    "    success_scores[dataset] = []                                   # create a success score vector \n",
    "    for frame in range(len(full_dict[dataset])):               # for every frame (by number, not the frame itself)     \n",
    "        success_scores[dataset].append(full_dict[dataset][frame][0])     # append the success score to the vector\n",
    "     \n",
    "    \n",
    "# by index determine wether a success score falls in under or over the 80th percentile\n",
    "\n",
    "for dataset in datasets:\n",
    "    success_scores[\"_\".join([dataset, 'ou'])] = np.repeat(\"NaN\", limits_80[dataset][1])\n",
    "    success_scores[\"_\".join([dataset, 'ou'])][:limits_80[dataset][0]] = 'U'    # up to the limit it is under = U\n",
    "    success_scores[\"_\".join([dataset, 'ou'])][limits_80[dataset][0]:] = 'O'    # Upwards of the limit it is over = O\n",
    "\n",
    "\n",
    "## by index determine the group a success score falls into (1-8)\n",
    "\n",
    "for dataset in datasets:\n",
    "    success_scores[\"_\".join([dataset, \"8\"])] = np.repeat(\"NaN\", limits_80[dataset][1])     # first create a vector with NaN (full length)\n",
    "    for i in range(len(limits_8[dataset])):                                               # fill it based on the read limits (s.above)\n",
    "        if i == 0:\n",
    "            success_scores[\"_\".join([dataset, \"8\"])][0:int(limits_8[dataset][i])] = i + 1    # between 0 and boarder of group 1\n",
    "        else:\n",
    "            success_scores[\"_\".join([dataset, \"8\"])][int(limits_8[dataset][i-1]):int(limits_8[dataset][i])] = i + 1  # between boarder of group -1 and group\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0360a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8837fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores['16m_100s_ou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores['16m_100s_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d91aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted (ascending)\n",
    "success_scores['16m_100s']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a63702",
   "metadata": {},
   "source": [
    "# Goals over and under 80th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29882cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_count = {}                                 # new dictionary to count the goals\n",
    "datasets = list(full_dict.keys())           # datasets in this case not split based on percentiles (area & interval)\n",
    "\n",
    "with open('Pickles/Boarder_values_10_e100.pkl', 'rb') as file:             # boarder values for the 80th percentile (the 8th element in each array)\n",
    "    percentiles_10 = p.load(file)\n",
    "    \n",
    "\n",
    "for dataset in datasets:\n",
    "    goal_count[\"_\".join([dataset, 'over80'])] = 0                    # for every dataset i count goals over\n",
    "    goal_count[\"_\".join([dataset, 'under80'])] = 0                   # and under --> 2 lists in dictionary\n",
    "    for frame in range(len(full_dict[dataset])):                   # every frame is checked\n",
    "        if dataset.startswith('30'):\n",
    "            if full_dict[dataset][frame][6] in goals_rel.time_string.values:        # to identify goals the time has to match\n",
    "                index_list = np.where(goals_rel[\"time_string\"].values == full_dict[dataset][frame][6])      # all those which match by the time stamp\n",
    "                for i in index_list[0]:\n",
    "                    if goals_rel.DFL_Name[i] in full_dict[dataset][frame][2] and goals_rel.Scoring_ha[i] == full_dict[dataset][frame][1] and goals_rel.Half[i] ==full_dict[dataset][frame][5]:       # have to also match by the match id and the team (scoring team) and the half (46 = HT1 vs 46 = HT2 min 1)\n",
    "                        if full_dict[dataset][frame][0] < percentiles_10[dataset][8]:                                               # if under the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + 1 # we add + 1 to under count\n",
    "                        elif full_dict[dataset][frame][0] > percentiles_10[dataset][8]:                                             # if over the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + 1   # we add +1  to over count\n",
    "                        elif full_dict[dataset][frame][0] == percentiles_10[dataset][8]:                                            # if excatly the boarder value\n",
    "                            tot_frequency = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])         # freqeuncy of frames with that success score\n",
    "                            over_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'O'))[0])         # freqeuncy of frames with that success score in over\n",
    "                            under_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'U'))[0])        # freqeuncy of frames with that success score in under\n",
    "                            o_goals = over_frequency / tot_frequency                                                              # goal share for over \n",
    "                            u_goals = under_frequency / tot_frequency                                                             # goal share for under\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + u_goals    # add the share\n",
    "                            #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + o_goals      # add the share\n",
    "        elif dataset.startswith('16'):\n",
    "            if full_dict[dataset][frame][6] in goals_rel.time_string[goals_rel.InBox==True].values:        # to identify goals the time has to match (==True --> from within the box)\n",
    "                #print(full_dict[dataset][frame])\n",
    "                index_list = np.where(goals_rel[\"time_string\"].values == full_dict[dataset][frame][6])      # all those which match by the time stamp\n",
    "                #print(index_list)\n",
    "                for i in index_list[0]:\n",
    "                    if goals_rel.DFL_Name[i] in full_dict[dataset][frame][2] and goals_rel.Scoring_ha[i] == full_dict[dataset][frame][1] and goals_rel.Half[i] ==full_dict[dataset][frame][5] and goals_rel.InBox[i]==True:       # have to also match by the match id and the team (scoring team) and the half and check inbox in case of two indeces in index list\n",
    "                        if full_dict[dataset][frame][0] < percentiles_10[dataset][8]:                                               # if under the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + 1 # we add + 1 to under count\n",
    "                        elif full_dict[dataset][frame][0] > percentiles_10[dataset][8]:                                             # if over the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + 1   # we add +1  to over count\n",
    "                        elif full_dict[dataset][frame][0] == percentiles_10[dataset][8]:                                            # if excatly the boarder value\n",
    "                            tot_frequency = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])         # freqeuncy of frames with that success score\n",
    "                            over_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'O'))[0])         # freqeuncy of frames with that success score in over\n",
    "                            under_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'U'))[0])        # freqeuncy of frames with that success score in under\n",
    "                            o_goals = over_frequency / tot_frequency                                                              # goal share for over \n",
    "                            u_goals = under_frequency / tot_frequency                                                             # goal share for under\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + u_goals    # add the share\n",
    "                            #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + o_goals      # add the share\n",
    "\n",
    "                        \n",
    "# create a dataframe storing the count of goals in a practical format and save to csv\n",
    "goal_count_80_df = pd.DataFrame() \n",
    "\n",
    "\n",
    "stri = ['over80', 'under80']\n",
    "strin = ['over', 'under']\n",
    "for dataset in datasets:\n",
    "    counts = []\n",
    "    for i in range(len(stri)):\n",
    "        counts.append(goal_count[\"_\".join([dataset, stri[i]])])\n",
    "        counts.append(len(split80_20[\"_\".join([dataset, strin[i]])]))        # add the length of the data for chi square\n",
    "    counts.append(len(success_scores[dataset]))\n",
    "    goal_count_80_df[dataset] = counts\n",
    "    \n",
    "\n",
    "\n",
    "goal_count_80_df.to_csv('Results/goal_Count_80_e100.csv')\n",
    "goal_count_80_df\n",
    "\n",
    "# goals over\n",
    "# data points over\n",
    "# goals under\n",
    "# data points under\n",
    "# all datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd42e6",
   "metadata": {},
   "source": [
    "# Goals in 8 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 8 groups split data\n",
    "with open('Pickles/8_split_e100.pkl', 'rb') as file:\n",
    "    split_8 = p.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_8.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25bbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331dae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_count_8 = {}\n",
    "datasets = list(full_dict.keys())\n",
    "\n",
    "\n",
    "with open('Pickles/Boarder_values_8_e100.pkl', 'rb') as file:\n",
    "    boarder_values_8 = p.load(file)\n",
    "    \n",
    "groups =list(range(1,9))\n",
    "\n",
    "tot_frequency_dict = {}\n",
    "group_frequencies_dict = {}\n",
    "goal_share = {}\n",
    "\n",
    "for dataset in datasets:                                   # for each combination of area & interval\n",
    "    for group in groups:                                   # for each group (by percentile) --> 1 to 8\n",
    "        goal_count_8[\"_\".join([dataset, str(group)])] = 0  # empty list in the dictionary for every group for every dataset\n",
    "        tot_frequency_dict[\"_\".join([dataset, str(group)])] = []         # dictionary for the total frequency of that success score\n",
    "        group_frequencies_dict[\"_\".join([dataset, str(group)])] = []     # dictionary for the freqeuncy of that success score in that group\n",
    "        goal_share[\"_\".join([dataset, str(group)])] = []                 # dictionary for the share of the goals scored at that success score for each group\n",
    "    for frame in range(len(full_dict[dataset])):       # for every frame (loop by number; not the frame itself)\n",
    "        if (full_dict[dataset][frame][6] in goals_rel.time_string.values and dataset.startswith('30')) or (full_dict[dataset][frame][6] in goals_rel.time_string[goals_rel.InBox==True].values):             # is the time in the frame in the list of goals (-->potential goal frame)\n",
    "            # either a 30m datapoint with the right time stamp or a 16m datapoint with the right time stamp matching only the ones in the box\n",
    "            index_list = np.where(goals_rel[\"time_string\"].values == full_dict[dataset][frame][6])     # create a list the index of the goal in the goal data frame\n",
    "            for i in index_list[0]:                                                                # for every goal with that time\n",
    "                if goals_rel.DFL_Name[i] in full_dict[dataset][frame][2] and goals_rel.Scoring_ha[i] == full_dict[dataset][frame][1] and goals_rel.Half[i] ==full_dict[dataset][frame][5] and (goals_rel.InBox[i]==True or dataset.startswith('30')):  # is the game name identical with the frame and the scoring team the correct one, correct half and in the box if not 30m dataset\n",
    "                    for group in groups:                   # for each group (by percentile) --> 1 to 8\n",
    "                        if group == 1:                                                   # if group is = 1 group-1 is not usable for lower limit so we here have to use 0 - group1 \n",
    "                            if full_dict[dataset][frame][0] < boarder_values_8[dataset][group-1]:                   # is the success score smaller than the limit of group1\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = goal_count_8[\"_\".join([dataset, str(group)])] + 1    # if yes it counts as a full goal to goals in group \n",
    "                            elif full_dict[dataset][frame][0] == boarder_values_8[dataset][group-1]:   # if not we check if it is exactly the boarder value; if yes we create a share based on the frequency of that success score in each group\n",
    "                                tot_frequency_dict[\"_\".join([dataset, str(group)])] = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])   # the tot_frequency is frequency of that success score across all groups\n",
    "                                #print(tot_frequency_dict[\"_\".join([dataset, str(group)])])\n",
    "                                group_frequencies_dict[\"_\".join([dataset, str(group)])] = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"8\"])]) == str(group)))[0])  # the freqeuncy of that success score in this group\n",
    "                                #print(group_frequencies_dict[\"_\".join([dataset, str(group)])])\n",
    "                                goal_share[\"_\".join([dataset, str(group)])] = group_frequencies_dict[\"_\".join([dataset, str(group)])]/tot_frequency_dict[\"_\".join([dataset, str(group)])]   # goal share = frequency in that group / overall frequency (* goals with that success score)\n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = float(goal_count_8[\"_\".join([dataset, str(group)])]) + goal_share[\"_\".join([dataset, str(group)])]      # finally we add that share to that group\n",
    "                                #print(goal_share[\"_\".join([dataset, str(group)])])\n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "                        else:\n",
    "                            if full_dict[dataset][frame][0] < boarder_values_8[dataset][group-1] and full_dict[dataset][frame][0] > boarder_values_8[dataset][group-2]: # if group is > 1 we can just check with limits (group -1 upper limit to group upper limit)\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = goal_count_8[\"_\".join([dataset, str(group)])] + 1                   # if yes it counts as a full goal to goals in group \n",
    "                            elif (full_dict[dataset][frame][0] == boarder_values_8[dataset][group-1]) | (full_dict[dataset][frame][0] == boarder_values_8[dataset][group-2]):                      # if not we check if it is exactly the boarder value; if yes we create a share based on the frequency of that success score in each group\n",
    "                                tot_frequency_dict[\"_\".join([dataset, str(group)])] = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])                           # the tot_frequency is frequency of that success score across all groups\n",
    "                                group_frequencies_dict[\"_\".join([dataset, str(group)])] = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (success_scores[\"_\".join([dataset, \"8\"])] == str(group)))[0]) # the tot_frequency is frequency of that success score across all groups\n",
    "                                goal_share[\"_\".join([dataset, str(group)])]= group_frequencies_dict[\"_\".join([dataset, str(group)])]/tot_frequency_dict[\"_\".join([dataset, str(group)])]           # goal share = frequency in that group / overall frequency (* goals with that success score)\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = float(goal_count_8[\"_\".join([dataset, str(group)])]) + goal_share[\"_\".join([dataset, str(group)])]                # finally we add that share to that group                                   \n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "\n",
    "                                \n",
    "# create a dataframe storing the count of goals in a practical format and save to csv\n",
    "goal_count_8_df = pd.DataFrame() \n",
    "\n",
    "for dataset in datasets:\n",
    "    counts = []\n",
    "    for i in list(range(1,9)):\n",
    "        counts.append(goal_count_8[\"_\".join([dataset, str(i)])])\n",
    "    goal_count_8_df[dataset] = counts\n",
    "    \n",
    "\n",
    "\n",
    "goal_count_8_df.to_csv('Results/goal_Count_8_e100.csv')\n",
    "goal_count_8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3d394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodlight_env",
   "language": "python",
   "name": "floodlight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
